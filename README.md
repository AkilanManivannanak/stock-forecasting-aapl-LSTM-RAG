

# Stock Forecasting System (AAPL) — LSTM + Baselines + FastAPI + Grounded RAG Copilot (Ollama)

Production-style **stock forecasting system** for **AAPL** that demonstrates an end-to-end ML workflow:
data ingestion → feature engineering → model training & evaluation (baseline-first) → artifacts/plots/report → API serving (FastAPI) → **strict grounded RAG copilot** answering questions about results and repo docs.

> This repository is built to showcase real AI Engineering behaviors: reproducible runs, baseline-first evaluation, tested API surface, and a grounded RAG assistant that refuses to hallucinate.

---

## What this project does

### Forecasting (FastAPI)
- Serves **next-day to 30-day** forecasts using a trained **LSTM** model.
- Provides a **multi-horizon** endpoint to request a bundle of horizons in one call.
- Produces demo-friendly outputs with date + predicted value.

### RAG Copilot (Grounded)
- Indexes project docs and reports under `data/kb/`.
- Persists a **Chroma** vector store under `artifacts/chroma` using collection `finance_copilot`.
- Uses embeddings: `sentence-transformers/all-MiniLM-L6-v2`.
- At query time, retrieves top-k chunks and answers **using only retrieved context**.
- If the answer isn’t explicitly present, returns exactly: **`Not found in docs.`**

---

## Tech stack
- **Python 3.12**
- **FastAPI** + **Uvicorn**
- **TensorFlow + Keras 3** (LSTM training/inference)
- **yfinance** (AAPL OHLCV ingestion)
- **NumPy / Pandas / scikit-learn**
- **pytest** (API tests)
- **LangChain + ChromaDB** (vector store)
- **sentence-transformers** embeddings
- **Ollama** (local LLM provider for RAG)

---

## Repo structure (high-level)
- `scripts/train.py` — trains LSTM + computes baselines + generates plots + writes experiment report
- `app.py` — FastAPI app (`/health`, `/forecast`, `/multi_horizon_forecast`, `/ask`)
- `src/` — data/features/models/plots/report utilities
- `src/rag_copilot/` — `ingest.py` builds Chroma index; `qa.py` answers questions with strict grounding
- `data/kb/` — docs and the generated `experiment_report.md`
- `tests/` — API tests (`pytest`)

---

## Dataset
- **Source:** Yahoo Finance via `yfinance`
- **Ticker:** AAPL
- **Data:** OHLCV time series
- **Start date:** configured in `config.py` (or your project config)

---

## Model evaluation (baseline-first)

We evaluate forecasting models using **RMSE** and publish results to:
- `data/kb/experiment_report.md` (generated by training)

Example (your numbers will reflect your latest run):
- Best model on RMSE often ends up being **naive**, which is expected for highly persistent price series.

**Why naive can win:** Close prices behave like a random walk; strong baselines can outperform complex models unless features, objectives (returns), and validation are designed correctly.

**Next improvements (engineering roadmap):**
- Predict **returns/log-returns** (directional usefulness), not raw close price only.
- Add walk-forward evaluation and regime slicing (bull/bear/sideways).
- Improve feature set and tune LSTM; compare against stronger baselines.

---

## Artifacts produced
Training produces:
- `models/` — saved trained model weights (LSTM)
- `outputs/metrics.csv` — RMSE leaderboard / evaluation metrics
- `plots/actual_vs_pred_lstm.png` — actual vs predicted plot on test split
- `data/kb/experiment_report.md` — markdown experiment report used by the RAG Copilot

---

## API endpoints

### Health
```bash
curl -s http://127.0.0.1:8000/health | python -m json.tool
```
### Forecast

curl -s -X POST http://127.0.0.1:8000/forecast \
  -H "Content-Type: application/json" \
  -d '{"ticker":"AAPL","days":5}' | python -m json.tool

### Multi-horizon forecast

curl -s -X POST http://127.0.0.1:8000/multi_horizon_forecast \
  -H "Content-Type: application/json" \
  -d '{"ticker":"AAPL","days":30}' | python -m json.tool

### RAG 

curl -s -X POST http://127.0.0.1:8000/ask \
  -H "Content-Type: application/json" \
  -d '{"ticker":"AAPL","question":"According to experiment_report.md, what is the best model on RMSE?","k":6}' \
  | python -m json.tool

---

## How to run 

# 1. Create venv + install deps

cd ~/stock-forecasting-llm-aapl
python -m venv .venv
source .venv/bin/activate
python -m pip install -U pip setuptools wheel
python -m pip install -r requirements.txt

# required for pandas markdown report rendering
python -m pip install -U tabulate

# required to avoid transformers/tensorflow keras3 issues in some stacks
python -m pip install -U tf-keras

# 2. Run tests

   python -m pytest -q


# 3. Train models + generate artifacts

   python -m scripts.train

# 4. Build RAG index (Chroma)

   rm -rf artifacts
mkdir -p artifacts/chroma

python -m src.rag_copilot.ingest \
  --ticker AAPL \
  --docs_dir data/kb \
  --persist_dir artifacts/chroma \
  --collection_name finance_copilot

python -c "from langchain_chroma import Chroma; from langchain_huggingface import HuggingFaceEmbeddings; emb=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2'); vs=Chroma(collection_name='finance_copilot', embedding_function=emb, persist_directory='artifacts/chroma'); print('CHROMA COUNT =', vs._collection.count())"

# 5. Run the API (ollama)

   Ensure Ollamam is running locally and you have the model pulled :

ollama serve
ollama pull llama3.1

Then:

export RAG_PROVIDER=ollama
export OLLAMA_BASE_URL=http://127.0.0.1:11434
export OLLAMA_MODEL=llama3.1
export RAG_PERSIST_DIR=$(pwd)/artifacts/chroma
export RAG_COLLECTION_NAME=finance_copilot
export HF_EMBED_MODEL=sentence-transformers/all-MiniLM-L6-v2

python -m uvicorn app:app --host 127.0.0.1 --port 8000

# open docs:

open http://127.0.0.1:8000/docs


# 6. 10 RAG Questions

cat > demo_questions.txt <<'EOF'
What is the goal of this project?
According to experiment_report.md, what is the best model on RMSE?
List the models and RMSE shown in experiment_report.md.
In RAG_PIPELINE.md, where is Chroma persisted and what collection name is used?
What embedding model is used for RAG?
Which folder is indexed during RAG ingestion?
What is the strict grounding rule for the RAG copilot?
Which FastAPI endpoints are available for forecasting?
Where are plots saved and which plot file is mentioned in experiment_report.md?
Where are future forecasts saved?
EOF

while IFS= read -r q; do
  echo
  echo "Q: $q"
  resp=$(curl -s -X POST http://127.0.0.1:8000/ask \
    -H "Content-Type: application/json" \
    -d "{\"ticker\":\"AAPL\",\"question\":\"$q\",\"k\":6}")

  echo "$resp" | python -m json.tool | head -n 60

OPTIONAL: macOS voice
ans=$(echo "$resp" | python -c "import sys,json; print(json.load(sys.stdin).get('answer',''))")
say -v Samantha "$ans"
done < demo_questions.txt
---

## Notes on grounded behavior:

The RAG assistant is intentionally strict:
- It answers only from retrieved data/kb/ context.
- It returns Not found in docs. when the answer is missing or not explicit.

This is designed to demonstrate safe, non-hallucinating retrieval behaviour.

---


